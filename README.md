# natural-language-processing
This repository will contain the jupyter notebooks consisting the code snipets for different NLP operations.
The first operation being tokenization. Tokenization is a process of splitting up a textual body into smaller units. In python or more specifically say in NLP
tokenization can be either sentence tokenization that breaks up a large textual body into sentences or, word tokenization that breaks up sentences into words.
